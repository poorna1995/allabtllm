![alt text](llm.png)

## LLM?

![alt text](LLM1.png)

### LLM Models

| Model Name      | #Parameters                                   | Release | Base Models | Open Source | Type            | #Tokens  | Training Dataset                                                                 | Architecture Explanation                                                                                                                                                                                |
| --------------- | --------------------------------------------- | ------- | ----------- | ----------- | --------------- | -------- | -------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **BERT**        | 110M, 340M                                    | 2018    | -           | ✓           | Encoder-Only    | 137B     | BooksCorpus, English Wikipedia                                                   | **Transformer Encoder**: BERT uses a bidirectional transformer encoder that processes input text in both directions simultaneously to capture context from both the left and right.                     |
| **RoBERTa**     | 355M                                          | 2019    | -           | ✓           | Encoder-Only    | 2.2T     | BooksCorpus, English Wikipedia, CC-NEWS, STORIES, Reddit                         | **Transformer Encoder**: RoBERTa is a robustly optimized variant of BERT, trained with more data and larger batch sizes for improved performance. It uses the same transformer encoder architecture.    |
| **ALBERT**      | 12M, 18M, 60M, 235M                           | 2019    | -           | ✓           | Encoder-Only    | 137B     | BooksCorpus, English Wikipedia                                                   | **Transformer Encoder**: ALBERT reduces the number of parameters by sharing weights and factorizing the embedding layer, improving efficiency while maintaining the same transformer encoder structure. |
| **DeBERTa**     | -                                             | 2020    | -           | ✓           | Encoder-Only    | -        | BooksCorpus, English Wikipedia, STORIES, Reddit content                          | **Enhanced Transformer Encoder**: DeBERTa improves BERT by incorporating a disentangled attention mechanism and an enhanced mask decoder for better context understanding.                              |
| **XLNet**       | 110M, 340M                                    | 2019    | -           | ✓           | Encoder-Only    | 32.89B   | BooksCorpus, English Wikipedia, Giga5, Common Crawl, ClueWeb 2012-B              | **Autoregressive Transformer**: XLNet combines the autoregressive training of GPT with the bidirectional context capture of BERT, using a permutation-based approach for input sequences.               |
| **GPT-1**       | 120M                                          | 2018    | -           | ✓           | Decoder-Only    | 1.3B     | BooksCorpus                                                                      | **Transformer Decoder**: GPT-1 uses a unidirectional autoregressive transformer decoder to predict the next word in a sequence, trained on a language modeling objective.                               |
| **GPT-2**       | 1.5B                                          | 2019    | -           | ✓           | Decoder-Only    | 10B      | Reddit outbound                                                                  | **Transformer Decoder**: GPT-2 expands on GPT-1 with more parameters and data, using the same unidirectional transformer decoder architecture but with better scalability and performance.              |
| **T5 (Base)**   | 223M                                          | 2019    | -           | ✓           | Encoder-Decoder | 156B     | Common Crawl                                                                     | **Encoder-Decoder**: T5 uses a standard encoder-decoder transformer architecture, where both the encoder and decoder are bidirectional, and it is trained for text-to-text tasks.                       |
| **MT5 (Base)**  | 300M                                          | 2020    | -           | ✓           | Encoder-Decoder | -        | New Common Crawl-based dataset in 101 languages                                  | **Multilingual Encoder-Decoder**: MT5 is a multilingual version of T5, employing the same architecture but trained on a variety of languages, making it suitable for multilingual tasks.                |
| **BART (Base)** | 139M                                          | 2019    | -           | ✓           | Encoder-Decoder | -        | Corrupting text                                                                  | **Encoder-Decoder with Denoising**: BART uses a denoising autoencoder approach, where the input text is corrupted, and the model is trained to reconstruct it using both the encoder and decoder.       |
| **GPT-3**       | 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13B, 175B | 2020    | -           | ×           | Decoder-Only    | 300B     | Common Crawl (filtered), WebText2, Books1, Books2, Wikipedia                     | **Transformer Decoder**: GPT-3 uses an autoregressive transformer decoder, scaled significantly from previous versions, allowing it to generate coherent and diverse text from prompt input.            |
| **CODEX**       | 12B                                           | 2021    | GPT         | ✓           | Decoder-Only    | -        | Public GitHub software repositories                                              | **Transformer Decoder**: Codex is a specialized version of GPT-3, trained on programming code and natural language text, using the same transformer decoder architecture for code generation.           |
| **WebGPT**      | 760M, 13B, 175B                               | 2021    | GPT-3       | ×           | Decoder-Only    | -        | ELI5                                                                             | **Transformer Decoder**: WebGPT is based on GPT-3 but with a web-browsing component, allowing it to retrieve information in real-time, improving its knowledge base for answering questions.            |
| **GPT-4**       | 1.76T                                         | 2023    | -           | ×           | Decoder-Only    | 13T      | -                                                                                | **Transformer Decoder**: GPT-4 is an advanced version of GPT-3, using an autoregressive transformer decoder architecture with more parameters and data for improved generation capabilities.            |
| **LLaMA1**      | 7B, 13B, 33B, 65B                             | 2023    | -           | ✓           | Decoder-Only    | 1T, 1.4T | Online sources                                                                   | **Transformer Decoder**: LLaMA models are designed to be efficient and competitive with GPT models, using a simple yet effective transformer decoder architecture.                                      |
| **LLaMA2**      | 7B, 13B, 34B, 70B                             | 2023    | -           | ✓           | Decoder-Only    | 2T       | Online sources                                                                   | **Transformer Decoder**: LLaMA2 builds on LLaMA1's architecture with optimizations for better performance, scaling to larger models for enhanced generative tasks.                                      |
| **LLaMA 3.1**   | 7B, 13B, 65B                                  | 2024    | -           | ✓           | Decoder-Only    | 3T       | Web data, research papers, books, code repositories, and conversational datasets | **Transformer Decoder**: LLaMA 3.1 is an advanced version of LLaMA, using an autoregressive transformer decoder optimized for large-scale data and more diverse training sources.                       |
| **Alpaca**      | 7B                                            | 2023    | LLaMA1      | ✓           | Decoder-Only    | -        | GPT-3.5                                                                          | **Transformer Decoder**: Alpaca is based on LLaMA1, designed for fine-tuning to generate conversational data efficiently, using the same architecture as LLaMA1.                                        |
| **Vicuna-13B**  | 13B                                           | 2023    | LLaMA1      | ✓           | Decoder-Only    | -        | GPT-3.5                                                                          | **Transformer Decoder**: Vicuna is a LLaMA-based model fine-tuned for dialogue generation, maintaining the LLaMA architecture while focusing on interactive tasks.                                      |
| **Koala**       | 13B                                           | 2023    | LLaMA       | ✓           | Decoder-Only    | -        | Dialogue data                                                                    | **Transformer Decoder**: Koala, based on LLaMA, is specifically fine-tuned for dialogue generation, designed to provide more natural and contextually appropriate conversational output.                |
| **Mistral-7B**  | 7.3B                                          | 2023    | -           | ✓           | Decoder-Only    | -        | -                                                                                | **Transformer Decoder**: Mistral-7B uses a standard transformer decoder architecture but is optimized for specific tasks like text generation with fewer parameters.                                    |
| **Code Llama**  | 34B                                           | 2023    | LLaMA2      | ✓           | Decoder-Only    | 500B     | Publicly available code                                                          | **Transformer Decoder**: Code Llama is built on LLaMA2 and is fine-tuned for generating code, leveraging the same transformer decoder architecture but specialized for programming tasks.               |
| **LongLLaMA**   | 3B, 7B                                        | 2023    | OpenLLaMA   | ✓           | Decoder-Only    | 1T       | -                                                                                | **Transformer Decoder**: LongLLaMA is a variation of LLaMA with a focus on handling long sequences efficiently, optimizing transformer decoder operations.                                              |
| **PaLM**        | 8B, 62B, 540B                                 | 2022    | -           | ×           | Decoder-Only    | 780B     | Web documents, books, Wikipedia, conversations, GitHub code                      | **Transformer Decoder**: PaLM                                                                                                                                                                           |

![alt text](image-2.png)

#### Evalutaion metric

| Benchmark Name                | Evaluation Metric                                                                               | Leaderboard                                                                     | Source                                                                      | paper with code                                                          |
| ----------------------------- | ----------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------- | --------------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| HumanEval                     | PASS@k                                                                                          | [Link](https://llm-leaderboard.streamlit.app/)                                  | [Link](https://github.com/openai/human-eval)                                | [Link](https://paperswithcode.com/sota/code-generation-on-humaneval)     |
| MBPP                          | PASS@k, Accuracy                                                                                | -                                                                               | [Link](https://github.com/google-research/google-research/tree/master/mbpp) | [Link](https://paperswithcode.com/sota/code-generation-on-mbpp)          |
| APPS                          | PASS@k, Accuracy                                                                                | -                                                                               | [Link](https://github.com/hendrycks/apps)                                   | [Link](https://paperswithcode.com/sota/code-generation-on-apps)          |
| CoNaLa                        | BLEU                                                                                            | [Link](#)                                                                       | [Link](#)                                                                   | -                                                                        |
| CodeParrot                    | PASS@k                                                                                          | [Link](#)                                                                       | -                                                                           | -                                                                        |
| HellaSwag                     | Accuracy                                                                                        | [Link](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) | [Link](https://rowanzellers.com/hellaswag/)                                 | [Link](https://paperswithcode.com/sota/sentence-completion-on-hellaswag) |
| AI2 Reasoning Challenge (ARC) | Accuracy                                                                                        | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| BoolQ                         | Accuracy                                                                                        | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| MultiRC                       | F1-score, Accuracy                                                                              | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| CNN/Daily Mail [200]          | Accuracy                                                                                        | -                                                                               | [Link](#)                                                                   | -                                                                        |
| SQuAD                         | F1-score, EM                                                                                    | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| RACE                          | Accuracy                                                                                        | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| CNN/Daily Mail [201]          | ROUGE                                                                                           | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| Drop                          | F1-score, EM                                                                                    | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| QuAC                          | F1-score, HEQ-Q, HEQ-D                                                                          | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| TriviaQA                      | EM, F1-score, Accuracy                                                                          | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| Natural Questions             | EM, F1-score, Accuracy                                                                          | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| StrategyQA                    | Accuracy, Recall@10, SARI                                                                       | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| CoQA                          | F1-score                                                                                        | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| XSum                          | ROUGE                                                                                           | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| SAMSum                        | ROUGE                                                                                           | -                                                                               | -                                                                           | [Link](#)                                                                |
| WikiSum                       | ROUGE                                                                                           | -                                                                               | [Link](#)                                                                   | -                                                                        |
| DialogSum                     | ROUGE                                                                                           | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| TruthfulQA                    | MC1, MC2, % true, % info, BLEURT                                                                | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| MMLU                          | Accuracy                                                                                        | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| GSM8K                         | Accuracy                                                                                        | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| PIQA                          | Accuracy                                                                                        | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| SIQA                          | Accuracy                                                                                        | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| OpenBookQA (OBQA)             | Accuracy                                                                                        | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| HotpotQA                      | EM, F1-score, Joint EM, Joint F1-score                                                          | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| MATH                          | Accuracy                                                                                        | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| CommonsenseQA                 | Accuracy                                                                                        | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| Natural Instructions          | ROUGE-L, Human                                                                                  | [Link](#)                                                                       | [Link](#)                                                                   | [Link](#)                                                                |
| BIG-bench                     | Accuracy, Average                                                                               | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| ToolTalk                      | Success rate, Precision, Recall, Incorrect action rate, Percent of failing error types          | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| MetaTool                      | Accuracy, Precision, Recall, F1-score                                                           | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| GPT4Tools                     | Success Rate of Thought, Action, Arguments, Overall Success                                     | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| API-Bank                      | Correctness, ROUGE, Error types (API Hallucination, Exceptions, Invalid Input Parameters, etc.) | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |
| Alpaca-CoT                    | -                                                                                               | -                                                                               | [Link](#)                                                                   | [Link](#)                                                                |

## LLM Categories

| Model         | Size       | #Params (B) | Type        | Availability | Origin   |
| ------------- | ---------- | ----------- | ----------- | ------------ | -------- |
| Davinci-002   | Very Large | 175         | Instruction | Unavailable  | Tuned    |
| Davinci-003   | Very Large | 175         | Instruction | Unavailable  | Tuned    |
| GPT 3.5-turbo | Large      | 20          | Chat        | Unavailable  | Tuned    |
| Falcon 7B     | Medium     | 7           | Foundation  | Public       | Original |
| Alpaca        | Large      | 13          | Chat        | Public       | Tuned    |
| Pythia 7B     | Medium     | 7           | Foundation  | Public       | Original |
| Pythia 12B    | Large      | 12          | Foundation  | Public       | Original |
| LLAMA 7B      | Medium     | 7           | Chat        | Public       | Original |
| LLAMA 2 7B    | Medium     | 7           | Chat        | Public       | Tuned    |
| LLAMA 2 7B    | Medium     | 7           | Foundation  | Public       | Original |
| Vicuna 13B    | Large      | 13          | Foundation  | Public       | Tuned    |
| Vicuna 7B     | Medium     | 7           | Foundation  | Public       | Tuned    |
| Claude        | Large      | 93          | Chat        | Unavailable  | Original |
| Claude 2      | Very Large | 137         | Chat        | Unavailable  | Original |
